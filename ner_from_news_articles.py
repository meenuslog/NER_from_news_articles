# -*- coding: utf-8 -*-
"""Task_4:NER from news articles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PsDD7BtqEuOLIUW4EKjV9ihSZhmdF1jV

# **Named Entity Recognition for News Articles**
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import shutil
import textwrap
from getpass import getpass

username = "meenuslog"
repo_name = "NER_from_news_articles"
token = getpass("Enter your GitHub token: ")

repo_url = f"https://{token}@github.com/{username}/{repo_name}.git"

# Now clone fresh
!git clone $repo_url


repo_path = f"/content/{repo_name}"
os.makedirs(f"{repo_path}/data", exist_ok=True)

# Move the IMDB dataset into repo's data directory
!cp "/content/news_articles.zip" "{repo_path}/data/news_articles.zip"

readme_content = textwrap.dedent("""

    # NER for News Articles

    This repository demonstrates Named Entity Recognition (NER) techniques on news articles using spaCy. It includes loading CoNLL formatted datasets, extracting named entities with pretrained and custom spaCy pipelines, and comparing different spaCy models.

    ## Requirements

    **install individually:**

    ```bash
    pip install pandas matplotlib seaborn spacy
    ```
""")

with open(f"{repo_path}/README.md", "w") as f:
    f.write(readme_content)

# Git config and commit
# %cd {repo_path}
!git config --global user.email "emaan.yawer.19@gmail.com"
!git config --global user.name "meenuslog"

!git add .
!git commit -m "Initial commit with README and dataset included"
!git push origin main

!wget https://raw.githubusercontent.com/meenuslog/NER_from_news_articles/refs/heads/main/data/news_articles.zip -O News_articles.zip

# Unzip the downloaded file
import zipfile

with zipfile.ZipFile("news_articles.zip", "r") as zip_ref:
    zip_ref.extractall("News Articles")

# Check the contents
import os
print(os.listdir("News Articles"))

"""###  Load CoNLL File Format into Python"""

def load_conll_file(filepath):
    data = []
    with open(filepath, "r", encoding="utf-8") as f:
        sentence = []
        for line in f:
            if line.strip():
                tokens = line.strip().split()
                if len(tokens) == 4:
                    sentence.append((tokens[0], tokens[-1]))  # word, ner_tag
            else:
                if sentence:
                    data.append(sentence)
                    sentence = []
    return data

# Load training data
train_path = "/content/NER_from_news_articles/News Articles/train.txt"
train_data = load_conll_file(train_path)
print("Total sentences:", len(train_data))
print("Sample sentence:", train_data[0])

import pandas as pd
import spacy
from spacy.pipeline import EntityRuler
from spacy import displacy
import os

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

def load_conll_file(filepath):
    data = []
    with open(filepath, "r", encoding="utf-8") as f:
        sentence = []
        for line in f:
            if line.strip():
                tokens = line.strip().split()
                if len(tokens) == 4:
                    sentence.append((tokens[0], tokens[-1]))  # word, ner_tag
            else:
                if sentence:
                    data.append(sentence)
                    sentence = []
    return data
train_data = load_conll_file("CoNLL003/train.txt")

print(f"Train: {len(train_data)}")

def extract_all_ner(nlp_model, dataset, source="unknown"):
    all_ents = []

    for sent in dataset:
        text = " ".join([word for word, tag in sent])
        doc = nlp_model(text)
        for ent in doc.ents:
            all_ents.append({
                "Entity": ent.text,
                "Label": ent.label_,
                "Sentence": text,
                "Source": source
            })
    return pd.DataFrame(all_ents)

"""### Extracting Entities Using spaCy's Pretrained Model"""

df_train_ents = extract_all_ner(nlp, train_data, "train")
df_all = pd.concat([df_train_ents], ignore_index=True)
print("Total Entities Extracted:", len(df_all))
df_all.head()

"""### Visualize Top Entities by NER Label"""

grouped = df_all.groupby("Label")["Entity"].value_counts().groupby(level=0).head(5)
print("Top 5 Entities per Label:\n")
print(grouped)

"""### Custom Rule-Based NER with EntityRuler"""

ruler = nlp.add_pipe("entity_ruler", before="ner")

patterns = [
    {"label": "ORG", "pattern": "OpenAI"},
    {"label": "GPE", "pattern": "Pakistan"},
    {"label": "PERSON", "pattern": "Elon Musk"},
    {"label": "ORG", "pattern": "United Nations"},
]
ruler.add_patterns(patterns)

"""###Test Custom EntityRuler with Sample Text"""

sample = "Elon Musk visited the United Nations headquarters in Pakistan. OpenAI was also there."
doc = nlp(sample)

for ent in doc.ents:
    print(ent.text, "->", ent.label_)
displacy.render(doc, style="ent", jupyter=True)

"""### Load and Compare spaCy Models (Small vs Transformer)"""

# Load both models
nlp_sm = spacy.load("en_core_web_sm")
# Download and load the transformer model
!python -m spacy download en_core_web_trf
nlp_trf = spacy.load("en_core_web_trf")

"""### Compare Entity Recognition: en_core_web_sm vs en_core_web_trf"""

from IPython.display import display, HTML

nlp_sm = spacy.load("en_core_web_sm")
nlp_trf = spacy.load("en_core_web_trf")

# Input sample text
sample = (
    "Elon Musk visited the United Nations headquarters in Pakistan. "
    "OpenAI was also there. Emmanuel Macron spoke at the World Economic Forum 2025 in Switzerland."
)
doc_sm = nlp_sm(sample)
doc_trf = nlp_trf(sample)

# Custom color scheme
colors = {
    "PERSON": "linear-gradient(90deg, #aa9cfc, #fc9ce7)",
    "ORG": "linear-gradient(90deg, #fbc2eb, #a6c1ee)",
    "GPE": "linear-gradient(90deg, #fddb92, #d1fdff)",
    "DATE": "linear-gradient(90deg, #f6d365, #fda085)",
    "LOC": "linear-gradient(90deg, #84fab0, #8fd3f4)",
    "NORP": "linear-gradient(90deg, #d4fc79, #96e6a1)",
    "EVENT": "linear-gradient(90deg, #ffecd2, #fcb69f)"
}
options = {"ents": list(colors.keys()), "colors": colors}

# Print plain text results
print("ðŸ”¹ SMALL Model (en_core_web_sm):")
for ent in doc_sm.ents:
    print(ent.text, "->", ent.label_)
print("\nðŸ”¹ TRANSFORMER Model (en_core_web_trf):")
for ent in doc_trf.ents:
    print(ent.text, "->", ent.label_)

# Render visual output
print("<h3 style='color:#6a1b9a;'>ðŸ”¹ en_core_web_sm</h3>")
displacy.render(doc_sm, style="ent", options=options, jupyter=True)
print("<h3 style='color:#2e7d32;'>ðŸ”¹ en_core_web_trf</h3>")
displacy.render(doc_trf, style="ent", options=options, jupyter=True)